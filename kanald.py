import cloudscraper
from bs4 import BeautifulSoup
import json
import re
import subprocess
import time

# Ayarlar
BASE_URL = "https://www.kanald.com.tr"
ARCHIVE_URL = "https://www.kanald.com.tr/diziler/arsiv?page="

def slugify(text):
    mapping = {'Ã§':'c','ÄŸ':'g','Ä±':'i','Ã¶':'o','ÅŸ':'s','Ã¼':'u','Ä°':'i'}
    text = text.lower()
    for tr, en in mapping.items(): text = text.replace(tr, en)
    return re.sub(r'[^a-z0-9]+', '-', text).strip('-')

def get_real_m3u8(scraper, bolum_url):
    """M3U8 linkini bulma mantÄ±ÄŸÄ±"""
    try:
        r1 = scraper.get(bolum_url, timeout=10)
        embed_match = re.search(r'<link[^>]+itemprop=["\']embedURL["\'][^>]+href=["\']([^"\']+)["\']', r1.text)
        if not embed_match: return bolum_url
        
        embed_url = embed_match.group(1)
        r2 = scraper.get(embed_url, timeout=10, headers={"Referer": BASE_URL})
        embed_html = r2.text
        
        patterns = [
            r'https?://vod[0-9]*\.cf\.dmcdn\.net/[^\s"\']+\.m3u8',
            r'https?://[^\s"\']+\.m3u8',
            r'["\']videoUrl["\']\s*:\s*["\']([^"\']+)["\']',
            r'src=["\']([^"\']+\.m3u8)["\']'
        ]
        
        for p in patterns:
            m = re.search(p, embed_html)
            if m:
                found_url = m.group(1) if "(" in p else m.group(0)
                return found_url.replace('\\/', '/')
        return embed_url
    except:
        return bolum_url

def commit_and_push(files):
    """DosyalarÄ± GitHub'a yÃ¼kler"""
    print(f"\nğŸ“¤ Dosyalar GitHub'a gÃ¶nderiliyor: {files}")
    try:
        subprocess.run(["git", "config", "--global", "user.name", "github-actions[bot]"], check=True)
        subprocess.run(["git", "config", "--global", "user.email", "github-actions[bot]@users.noreply.github.com"], check=True)
        
        for f in files:
            subprocess.run(["git", "add", f], check=True)
            
        if subprocess.run(["git", "status", "--porcelain"], capture_output=True, text=True).stdout:
            subprocess.run(["git", "commit", "-m", "ğŸ”„ Kanal D Playlist GÃ¼ncellendi (JSON & M3U)"], check=True)
            subprocess.run(["git", "push"], check=True)
            print("ğŸš€ GitHub'a yÃ¼klendi!")
        else:
            print("âœ¨ DeÄŸiÅŸiklik yok, yÃ¼kleme yapÄ±lmadÄ±.")
    except Exception as e: print(f"âŒ Git HatasÄ±: {e}")

def create_outputs(series_data):
    files_to_push = []

    # 1. JSON OLUÅTURMA
    json_filename = "kanald.json"
    with open(json_filename, "w", encoding="utf-8") as f:
        json.dump(series_data, f, indent=4, ensure_ascii=False)
    files_to_push.append(json_filename)
    print(f"âœ… {json_filename} oluÅŸturuldu. Toplam Dizi: {len(series_data)}")

    # 2. M3U OLUÅTURMA
    m3u_filename = "kanald.m3u"
    with open(m3u_filename, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        
        for dizi_id, data in series_data.items():
            dizi_adi = dizi_id.replace("-", " ").title()
            resim = data.get("resim", "")
            
            for bolum in data['bolumler']:
                baslik = bolum['ad']
                link = bolum['link']
                f.write(f'#EXTINF:-1 group-title="{dizi_adi}" tvg-logo="{resim}", {baslik}\n')
                f.write(f'{link}\n')
                
    files_to_push.append(m3u_filename)
    print(f"âœ… {m3u_filename} oluÅŸturuldu.")

    commit_and_push(files_to_push)

def run_scraper():
    print("ğŸš€ Kanal D Scraper BaÅŸlatÄ±ldÄ± (GeliÅŸmiÅŸ Mod)...")
    scraper = cloudscraper.create_scraper(browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True})
    series_data = {}

    # Sayfa sayÄ±sÄ±nÄ± artÄ±rabilirsin (1, 11) gibi
    for page in range(1, 6): 
        print(f"\nğŸ“„ Sayfa {page} taranÄ±yor...")
        try:
            resp = scraper.get(f"{ARCHIVE_URL}{page}", timeout=15)
            soup = BeautifulSoup(resp.text, 'html.parser')
            cards = soup.select('a.poster-card, .card-item, .item') # Ana liste seÃ§icileri
            
            if not cards: 
                print("âš ï¸ Bu sayfada dizi bulunamadÄ±.")
                break

            for card in cards:
                title_tag = card.get('title') or card.find('img').get('alt') if card.find('img') else None
                if not title_tag: continue
                
                title = str(title_tag)
                href = card.get('href')
                dizi_id = slugify(title)

                full_url = BASE_URL + href if href.startswith('/') else href
                print(f"  ğŸ“º {title} kontrol ediliyor...")

                # BÃ¶lÃ¼m sayfasÄ±na git
                b_url_main = full_url.rstrip('/') + "/bolumler"
                b_resp = scraper.get(b_url_main)
                b_soup = BeautifulSoup(b_resp.text, 'html.parser')

                # PLAN A: Standart Kartlar
                b_cards = b_soup.select('.story-card, .content-card, .video-card, .media-block, .card-item')
                
                # PLAN B: EÄŸer standart kart yoksa, tÃ¼m linkleri tara
                if not b_cards:
                    all_links = b_soup.find_all('a', href=True)
                    # Ä°Ã§inde 'bolum' geÃ§en ve 'fragman' olmayan linkleri al
                    b_cards = [
                        a for a in all_links 
                        if '/bolum/' in a['href'] 
                        and ('izle' in a['href'] or 'bolum' in a.get_text().lower())
                        and 'fragman' not in a['href']
                    ]
                    # Tekrar edenleri temizle (set kullanarak)
                    seen = set()
                    unique_cards = []
                    for c in b_cards:
                        h = c['href']
                        if h not in seen:
                            seen.add(h)
                            unique_cards.append(c)
                    b_cards = unique_cards

                print(f"    ğŸ” {len(b_cards)} adet bÃ¶lÃ¼m bulundu.")

                eps = []
                for bc in b_cards[:10]: # Her diziden son 10 bÃ¶lÃ¼m
                    link_tag = bc if bc.name == 'a' else bc.find('a', href=True)
                    
                    # Ä°sim bulmaya Ã§alÄ±ÅŸ
                    name_tag = bc.select_one('.title, h3, h2, .description') 
                    name_text = name_tag.get_text(strip=True) if name_tag else ""
                    
                    # EÄŸer isim yoksa linkin title'Ä±na bak veya link metnini al
                    if not name_text and link_tag:
                        name_text = link_tag.get('title') or link_tag.get_text(strip=True)

                    if link_tag:
                        target_url = BASE_URL + link_tag['href'] if link_tag['href'].startswith('/') else link_tag['href']
                        m3u8_link = get_real_m3u8(scraper, target_url)
                        
                        # BÃ¶lÃ¼m adÄ±nÄ± temizle
                        clean_name = f"{title} - {name_text}" if title not in name_text else name_text
                        
                        eps.append({"ad": clean_name, "link": m3u8_link})

                if eps:
                    img = card.find('img')
                    poster = img.get('data-src') or img.get('src', '') if img else ""
                    series_data[dizi_id] = {"resim": poster, "bolumler": eps[::-1]}

        except Exception as e:
            print(f"âŒ Hata: {e}")
            continue

    if series_data:
        create_outputs(series_data)
    else:
        print("\nâš ï¸ HiÃ§bir veri Ã§ekilemedi. SeÃ§icileri veya site yapÄ±sÄ±nÄ± kontrol et.")

if __name__ == "__main__":
    run_scraper()

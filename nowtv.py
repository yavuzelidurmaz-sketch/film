import cloudscraper
from bs4 import BeautifulSoup
import re
import time

# --- AYARLAR ---
BASE_URL = "https://www.nowtv.com.tr"

# Taranacak Kaynaklar
TARGET_URLS = [
    {"url": "/dizi-izle", "type": "DIZI"},
    {"url": "/dizi-arsivi", "type": "DIZI"},
    {"url": "/program-izle", "type": "PROGRAM"},
    {"url": "/program-arsivi", "type": "PROGRAM"}
]

def get_soup(scraper, url):
    """URL'den BeautifulSoup nesnesi dÃ¶ndÃ¼rÃ¼r."""
    try:
        resp = scraper.get(BASE_URL + url if not url.startswith('http') else url, timeout=15)
        return BeautifulSoup(resp.text, 'html.parser')
    except Exception as e:
        print(f"    Hata (Get Soup): {e}")
        return None

def get_shows_from_category(scraper, category_url):
    """Kategori sayfasÄ±ndaki dizi/program kartlarÄ±nÄ± bulur."""
    shows = []
    print(f"\nğŸ“‚ Kategori TaranÄ±yor: {category_url}")
    soup = get_soup(scraper, category_url)
    
    if not soup:
        return []

    # NOW TV genelde kartlarÄ± 'el-item' veya genel 'a' etiketleri iÃ§inde resimle sunar
    # GeniÅŸ kapsamlÄ± bir tarama yapalÄ±m
    links = soup.find_all('a', href=True)
    
    seen_slugs = set()

    for link in links:
        href = link['href']
        
        # Sadece dizi/program linklerini filtrele
        # Genellikle /dizi-adi veya /program-adi ÅŸeklindedir ve resim iÃ§erir
        img = link.find('img')
        
        if img and href.count('/') == 1 and len(href) > 2: # Basit slug kontrolÃ¼
            if href in seen_slugs: continue
            
            # YasaklÄ± kelimeler
            if any(x in href for x in ['yayin-akisi', 'canli-yayin', 'haber', 'iletisim', 'kunye']):
                continue

            title = img.get('alt') or img.get('title') or href.strip('/').replace('-', ' ').title()
            poster = img.get('data-src') or img.get('src') or ""
            
            seen_slugs.add(href)
            
            shows.append({
                "title": title.strip(),
                "url": href, # /kizil-goncalar gibi
                "poster": poster
            })
            
    print(f"  -> {len(shows)} iÃ§erik bulundu.")
    return shows

def get_episodes(scraper, show_url):
    """Dizinin bÃ¶lÃ¼mler sayfasÄ±ndaki Select Box'tan linkleri Ã§eker"""
    episodes = []
    # BÃ¶lÃ¼mler sayfasÄ± genelde /dizi-adi/bolumler ÅŸeklindedir
    bolumler_url = f"{show_url}/bolumler"
    
    soup = get_soup(scraper, bolumler_url)
    if not soup: return []

    # Select box'Ä± bul (NOW TV yapÄ±sÄ±)
    select_box = soup.find('select', id='video-finder-changer')
    
    if select_box:
        options = select_box.find_all('option', {'data-target': True})
        
        # Sayfadaki tÃ¼m m3u8 linklerini de Ã¶nbelleÄŸe alalÄ±m (Regex ile)
        # Bazen data-target bir HTML sayfasÄ±dÄ±r, m3u8 o sayfanÄ±n iÃ§indedir.
        # Bazen de direkt m3u8 linkidir.
        
        for opt in options:
            ep_name = opt.get_text(strip=True)
            target_link = opt['data-target'] # Bu bazen m3u8, bazen izleme sayfasÄ± linkidir
            
            real_link = None
            
            # 1. EÄŸer link direkt m3u8 ise
            if ".m3u8" in target_link:
                real_link = target_link
            else:
                # 2. DeÄŸilse, o sayfaya gidip m3u8 ara (Deep Scan)
                # Ancak her bÃ¶lÃ¼m iÃ§in istek atmak yavaÅŸlatÄ±r. 
                # NOW TV'de genelde data-target iÃ§indeki sayfa aÃ§Ä±lÄ±nca m3u8 regex ile bulunur.
                try:
                    # HÄ±z optimizasyonu: EÄŸer Ã§ok bÃ¶lÃ¼m varsa hepsine istek atma, sadece son 5'e at
                    # Veya hepsine at (uzun sÃ¼rer). Åimdilik hepsini deneyelim.
                    real_link = get_m3u8_from_page(scraper, target_link)
                except:
                    pass
            
            if real_link:
                episodes.append({
                    "name": ep_name,
                    "url": real_link
                })
    
    return episodes

def get_m3u8_from_page(scraper, url):
    """Tekil video sayfasÄ±ndan m3u8 regex ile Ã§eker"""
    try:
        # Url tam deÄŸilse tamamla
        full_url = BASE_URL + url if not url.startswith('http') else url
        r = scraper.get(full_url, timeout=5)
        
        # Regex: https://... .m3u8
        match = re.search(r'https?://[^\s"\'\\,]+\.m3u8[^\s"\'\\,]*', r.text)
        if match:
            return match.group(0).replace('\\/', '/')
    except:
        pass
    return None

def create_m3u(data):
    filename = "nowtv.m3u"
    print(f"\nğŸ“ {filename} dosyasÄ± oluÅŸturuluyor...")
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        
        for show in data:
            group = show['type']
            title = show['title']
            poster = show['poster']
            
            for ep in show['episodes']:
                ep_name = ep['name']
                link = ep['url']
                
                full_title = f"{title} - {ep_name}"
                
                # M3U FormatÄ±
                f.write(f'#EXTINF:-1 group-title="{group}" tvg-logo="{poster}",{full_title}\n')
                f.write(f'{link}\n')
                
    print("âœ… M3U TamamlandÄ±!")

def run_scraper():
    print("ğŸš€ NOW TV Scraper BaÅŸlatÄ±ldÄ± (M3U Modu)...")
    scraper = cloudscraper.create_scraper(browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True})
    
    final_data = []

    for target in TARGET_URLS:
        shows = get_shows_from_category(scraper, target['url'])
        
        for i, show in enumerate(shows):
            print(f"  ğŸ“º [{i+1}/{len(shows)}] Ä°ÅŸleniyor: {show['title']}")
            
            episodes = get_episodes(scraper, show['url'])
            
            if episodes:
                # BÃ¶lÃ¼mleri ekle
                final_data.append({
                    "title": show['title'],
                    "type": target['type'],
                    "poster": show['poster'],
                    "episodes": episodes
                })
                print(f"     + {len(episodes)} bÃ¶lÃ¼m eklendi.")
            else:
                print(f"     - BÃ¶lÃ¼m bulunamadÄ±.")
                
    create_m3u(final_data)

if __name__ == "__main__":
    run_scraper()
